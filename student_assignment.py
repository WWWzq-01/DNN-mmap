#!/usr/bin/env python3

"""
å­¦ç”Ÿä½œä¸šï¼šå®ç°Memory-Mappedæ•°æ®é›†
æ‰§è¡Œï¼š
```
python student_assignment.py
```
ğŸ¯ ä½œä¸šç›®æ ‡ï¼š
åŸºäºä¼ ç»Ÿçš„TraditionalDatasetå®ç°ï¼Œåˆ›å»ºä¸€ä¸ªå†…å­˜å‹å¥½çš„MmapDatasetç±»

ğŸ“‹ æäº¤è¦æ±‚ï¼š
1. å®Œæˆä¸‹é¢çš„ MmapDataset ç±»å®ç°
2. è¿è¡Œæµ‹è¯•å¯¹æ¯”ä¸¤ç§æ–¹å¼çš„å†…å­˜ä½¿ç”¨

"""

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import os
import time
import psutil
import pickle
import mmap
import struct
import gc


def get_memory_mb():
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    return psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024


class TraditionalDataset(Dataset):
    """
    ä¼ ç»Ÿå®ç°ï¼ˆå·²å®Œæˆï¼Œæ— éœ€ä¿®æ”¹ï¼‰
    é—®é¢˜ï¼šä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼Œå ç”¨å¤§é‡å†…å­˜
    """
    
    def __init__(self, data_file, labels_file):
        print("ğŸ”„ ä¼ ç»Ÿæ–¹å¼ï¼šæ­£åœ¨å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜...")
        
        with open(data_file + '.meta', 'rb') as f:
            self.metadata = pickle.load(f)
        
        self.num_samples = self.metadata['num_samples']
        self.data_shape = self.metadata['data_shape']
        
        # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        with open(data_file, 'rb') as f:
            data_bytes = f.read()
        with open(labels_file, 'rb') as f:
            labels_bytes = f.read()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå†è½¬ä¸ºtensor
        total_shape = (self.num_samples,) + self.data_shape
        data_array = np.frombuffer(data_bytes, dtype=np.float32).reshape(total_shape)
        labels_array = np.frombuffer(labels_bytes, dtype=np.int32)
        
        self.data = torch.from_numpy(data_array.copy())
        self.labels = torch.from_numpy(labels_array.copy())
        
        print(f"âœ… ä¼ ç»Ÿæ–¹å¼åŠ è½½å®Œæˆï¼š{self.data.shape}")
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]


class MmapDataset(Dataset):
    """
    ğŸš§ ã€å­¦ç”Ÿå®ç°åŒºåŸŸã€‘Memory-Mappedæ•°æ®é›†å®ç°
    
    ğŸ’¡ å®ç°æç¤ºï¼š
    1. ä½¿ç”¨mmapæ¨¡å—åˆ›å»ºå†…å­˜æ˜ å°„ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è¯»å–æ–‡ä»¶
    2. å®ç°æ‡’åŠ è½½ï¼šåªåœ¨__getitem__ä¸­æŒ‰éœ€è¯»å–æ•°æ®
    3. ä½¿ç”¨np.frombuffer()ç›´æ¥ä»mmapåˆ›å»ºnumpyæ•°ç»„è§†å›¾ï¼ˆé›¶æ‹·è´ï¼‰
    4. æ­£ç¡®è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡
    
    ğŸ“š éœ€è¦çš„æ¨¡å—ï¼š
    - mmap: åˆ›å»ºå†…å­˜æ˜ å°„
    - os: æ‰“å¼€æ–‡ä»¶æè¿°ç¬¦
    - struct: è§£æäºŒè¿›åˆ¶æ•°æ®ï¼ˆæ ‡ç­¾ï¼‰
    - numpy: åˆ›å»ºæ•°ç»„è§†å›¾
    """
    
    def __init__(self, data_file, labels_file):
        """
        ğŸ¯ TODO 1: åˆå§‹åŒ–MmapDataset
        
        è¦æ±‚ï¼š
        1. è¯»å–å…ƒæ•°æ®æ–‡ä»¶ (data_file + '.meta')
        2. ä½¿ç”¨os.open()ä»¥åªè¯»æ¨¡å¼æ‰“å¼€æ•°æ®æ–‡ä»¶å’Œæ ‡ç­¾æ–‡ä»¶
        3. ä½¿ç”¨mmap.mmap()åˆ›å»ºå†…å­˜æ˜ å°„
        4. ä¿å­˜å¿…è¦çš„ä¿¡æ¯ç”¨äºåç»­æ•°æ®è®¿é—®
        
        æç¤ºï¼š
        - self.metadata = pickle.load(...)
        metadata = {
        'num_samples': num_samples,
        'data_shape': image_shape,
        'sample_size': sample_size
        }
        - self._data_fd = os.open(data_file, os.O_RDONLY)
        - self._data_mmap = mmap.mmap(fd, 0, access=mmap.ACCESS_READ)
        """
        print("ğŸš€ Mmapæ–¹å¼ï¼šåˆ›å»ºå†…å­˜æ˜ å°„ï¼ˆä¸åŠ è½½æ•°æ®åˆ°å†…å­˜ï¼‰...")
        
        # TODO: åœ¨è¿™é‡Œå®ç°åˆå§‹åŒ–ä»£ç 
        # æ­¥éª¤1: è¯»å–å…ƒæ•°æ®
        # æ­¥éª¤2: æ‰“å¼€æ–‡ä»¶å¹¶åˆ›å»ºå†…å­˜æ˜ å°„
        # æ­¥éª¤3: ä¿å­˜å¿…è¦çš„ä¿¡æ¯
        
        pass  # åˆ é™¤è¿™è¡Œï¼Œå†™å…¥ä½ çš„å®ç°
    
    def __len__(self):
        """
        ğŸ¯ TODO 2: è¿”å›æ•°æ®é›†å¤§å°
        
        æç¤ºï¼šä»å…ƒæ•°æ®ä¸­è·å–æ ·æœ¬æ•°é‡
        """
        pass  # åˆ é™¤è¿™è¡Œï¼Œå†™å…¥ä½ çš„å®ç°
    
    def __getitem__(self, idx):
        """
        ğŸ¯ TODO 3: è·å–å•ä¸ªæ ·æœ¬ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰
        
        è¦æ±‚ï¼š
        1. è®¡ç®—æ ·æœ¬idxåœ¨æ•°æ®æ–‡ä»¶ä¸­çš„å­—èŠ‚åç§»é‡
        2. ä½¿ç”¨np.frombuffer()ä»å†…å­˜æ˜ å°„åˆ›å»ºæ•°ç»„è§†å›¾ï¼ˆé›¶æ‹·è´ï¼‰
        3. è¯»å–å¯¹åº”çš„æ ‡ç­¾
        4. è½¬æ¢ä¸ºPyTorch tensorå¹¶è¿”å›
        
        æç¤ºï¼š
        - æ•°æ®åç§»é‡ = idx * sample_sizeï¼ˆå­—èŠ‚ï¼‰
        - æ ‡ç­¾åç§»é‡ = idx * 4ï¼ˆint32 = 4å­—èŠ‚ï¼‰
        - np.frombuffer(mmap, dtype=np.float32, count=..., offset=...)
        - struct.unpack('i', label_bytes)[0] æˆ– np.frombuffer()è§£ææ ‡ç­¾
        """
        pass  # åˆ é™¤è¿™è¡Œï¼Œå†™å…¥ä½ çš„å®ç°
    
    def __del__(self):
        """
        ğŸ¯ TODO 4: æ¸…ç†èµ„æº
        
        è¦æ±‚ï¼š
        1. å…³é—­å†…å­˜æ˜ å°„
        2. å…³é—­æ–‡ä»¶æè¿°ç¬¦
        3. ä½¿ç”¨try-excepté˜²æ­¢é‡å¤å…³é—­çš„é”™è¯¯
        """
        pass  # åˆ é™¤è¿™è¡Œï¼Œå†™å…¥ä½ çš„å®ç°


def create_test_dataset(num_samples=8000, image_shape=(3, 64, 64)):
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆå·²å®Œæˆï¼Œæ— éœ€ä¿®æ”¹ï¼‰"""
    data_file = 'student_test_data.bin'
    labels_file = 'student_test_labels.bin'
    
    print(f"ğŸ“ åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼š{num_samples} ä¸ªæ ·æœ¬ï¼Œå½¢çŠ¶ {image_shape}")
    
    sample_size = np.prod(image_shape) * 4
    total_size_mb = num_samples * sample_size / 1024 / 1024
    print(f"   é¢„è®¡å¤§å°ï¼š{total_size_mb:.1f} MB")
    
    # åˆ†æ‰¹å†™å…¥æ•°æ®
    batch_size = 1000
    np.random.seed(42)
    
    with open(data_file, 'wb') as f:
        for i in range(0, num_samples, batch_size):
            end_idx = min(i + batch_size, num_samples)
            batch_data = np.random.randn(end_idx - i, *image_shape).astype(np.float32)
            f.write(batch_data.tobytes())
    
    # å†™å…¥æ ‡ç­¾
    labels = np.random.randint(0, 10, num_samples, dtype=np.int32)
    with open(labels_file, 'wb') as f:
        f.write(labels.tobytes())
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'num_samples': num_samples,
        'data_shape': image_shape,
        'sample_size': sample_size
    }
    with open(data_file + '.meta', 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"âœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ")
    return data_file, labels_file, total_size_mb


def test_dataset(dataset_class, data_file, labels_file, name):
    """æµ‹è¯•æ•°æ®é›†æ€§èƒ½ï¼ˆå·²å®Œæˆï¼Œæ— éœ€ä¿®æ”¹ï¼‰"""
    print(f"\n{'='*50}")
    print(f"ğŸ§ª æµ‹è¯• {name}")
    print(f"{'='*50}")
    
    gc.collect()
    initial_memory = get_memory_mb()
    
    # åˆ›å»ºæ•°æ®é›†
    start_time = time.time()
    try:
        dataset = dataset_class(data_file, labels_file)
        init_time = time.time() - start_time
        
        after_init_memory = get_memory_mb()
        init_memory_increase = after_init_memory - initial_memory
        
        print(f"â±ï¸  åˆå§‹åŒ–æ—¶é—´ï¼š{init_time:.2f} ç§’")
        print(f"ğŸ“ˆ åˆå§‹åŒ–å†…å­˜å¢é•¿ï¼š{init_memory_increase:.1f} MB")
        
        # æµ‹è¯•æ•°æ®è®¿é—®
        dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)
        
        max_memory = after_init_memory
        for i, (data, labels) in enumerate(dataloader):
            current_memory = get_memory_mb()
            max_memory = max(max_memory, current_memory)
            
            if i >= 30:  # æµ‹è¯•30ä¸ªbatch
                break
            
            if i % 10 == 0:
                print(f"   æ‰¹æ¬¡ {i:2d}: å†…å­˜ {current_memory:6.1f} MB, æ•°æ®å½¢çŠ¶ {data.shape}")
        
        total_memory_increase = max_memory - initial_memory
        
        print(f"ğŸ“Š {name} ç»“æœï¼š")
        print(f"   ğŸ“ˆ æ€»å†…å­˜å¢é•¿ï¼š{total_memory_increase:.1f} MB")
        print(f"   ğŸ” å³°å€¼å†…å­˜ï¼š{max_memory:.1f} MB")
        
        # æ¸…ç†
        del dataset, dataloader
        gc.collect()
        
        return {
            'init_time': init_time,
            'memory_increase': total_memory_increase,
            'peak_memory': max_memory
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼š{e}")
        print(f"ğŸ’¡ æ£€æŸ¥ä½ çš„ {dataset_class.__name__} å®ç°")
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ“ å­¦ç”Ÿä½œä¸šï¼šMemory-Mappedæ•°æ®é›†å®ç°")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    data_file, labels_file, file_size = create_test_dataset(num_samples=6000)
    print(f"ğŸ“ æµ‹è¯•æ•°æ®å¤§å°ï¼š{file_size:.1f} MB")
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹å¼
    traditional_result = test_dataset(TraditionalDataset, data_file, labels_file, "ä¼ ç»Ÿæ•°æ®é›†")
    
    # æµ‹è¯•Mmapæ–¹å¼
    mmap_result = test_dataset(MmapDataset, data_file, labels_file, "Mmapæ•°æ®é›†")
    
    # å¯¹æ¯”ç»“æœ
    if traditional_result and mmap_result:
        print(f"\n{'='*60}")
        print("ğŸ“Š å¯¹æ¯”ç»“æœ")
        print(f"{'='*60}")
        
        print(f"ğŸ“ æ•°æ®é›†å¤§å°ï¼š{file_size:.1f} MB")
        print(f"ğŸ“ˆ ä¼ ç»Ÿæ–¹å¼å†…å­˜ï¼š{traditional_result['memory_increase']:.1f} MB")
        print(f"ğŸ“ˆ Mmapæ–¹å¼å†…å­˜ï¼š{mmap_result['memory_increase']:.1f} MB")
        
        if mmap_result['memory_increase'] > 0:
            memory_saved = traditional_result['memory_increase'] - mmap_result['memory_increase']
            percentage_saved = (memory_saved / traditional_result['memory_increase']) * 100
            print(f"ğŸ’¾ å†…å­˜èŠ‚çœï¼š{memory_saved:.1f} MB ({percentage_saved:.1f}%)")
            
            if percentage_saved > 30:
                print("ğŸ‰ ä¼˜ç§€ï¼ä½ çš„Mmapå®ç°æ˜¾è‘—èŠ‚çœäº†å†…å­˜ï¼")
            elif percentage_saved > 10:
                print("âœ… ä¸é”™ï¼ä½ çš„Mmapå®ç°èŠ‚çœäº†ä¸€äº›å†…å­˜")
            else:
                print("ğŸ¤” Mmapå®ç°å¯èƒ½è¿˜æœ‰ä¼˜åŒ–ç©ºé—´")
        
        print(f"\nğŸ¯ ä½œä¸šè¯„ä¼°ï¼š")
        if mmap_result['memory_increase'] < traditional_result['memory_increase']:
            print("âœ… Mmapå®ç°æˆåŠŸå‡å°‘äº†å†…å­˜ä½¿ç”¨")
        else:
            print("âŒ Mmapå®ç°æ²¡æœ‰å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œè¯·æ£€æŸ¥å®ç°")
    
    # æ¸…ç†æ–‡ä»¶
    for f in ['student_test_data.bin', 'student_test_labels.bin', 'student_test_data.bin.meta']:
        try:
            os.remove(f)
        except:
            pass



if __name__ == "__main__":
    main()
